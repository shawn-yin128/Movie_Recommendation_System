{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# set up packages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"moive analysis\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "location = \"D:/py_movie_recommendation_system/data/\"\n",
    "movies_df = spark.read.load(location+\"movies.csv\", format='csv', header = True)\n",
    "ratings_df = spark.read.load(location+\"ratings.csv\", format='csv', header = True)\n",
    "links_df = spark.read.load(location+\"links.csv\", format='csv', header = True)\n",
    "tags_df = spark.read.load(location+\"tags.csv\", format='csv', header = True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# data preprocessing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# drop useless column\n",
    "movie_ratings=ratings_df.drop('timestamp')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Data type convert\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "movie_ratings = movie_ratings.withColumn(\"userId\", movie_ratings[\"userId\"].cast(IntegerType()))\n",
    "movie_ratings = movie_ratings.withColumn(\"movieId\", movie_ratings[\"movieId\"].cast(IntegerType()))\n",
    "movie_ratings = movie_ratings.withColumn(\"rating\", movie_ratings[\"rating\"].cast(FloatType()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+\n",
      "|userId|movieId|rating|\n",
      "+------+-------+------+\n",
      "|     1|      1|   4.0|\n",
      "|     1|      3|   4.0|\n",
      "|     1|      6|   4.0|\n",
      "|     1|     47|   5.0|\n",
      "|     1|     50|   5.0|\n",
      "|     1|     70|   3.0|\n",
      "|     1|    101|   5.0|\n",
      "|     1|    110|   4.0|\n",
      "|     1|    151|   5.0|\n",
      "|     1|    157|   5.0|\n",
      "|     1|    163|   5.0|\n",
      "|     1|    216|   5.0|\n",
      "|     1|    223|   3.0|\n",
      "|     1|    231|   5.0|\n",
      "|     1|    235|   4.0|\n",
      "|     1|    260|   5.0|\n",
      "|     1|    296|   3.0|\n",
      "|     1|    316|   3.0|\n",
      "|     1|    333|   5.0|\n",
      "|     1|    349|   4.0|\n",
      "+------+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movie_ratings.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# train test split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# import package\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import CrossValidator,ParamGridBuilder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Create test and train set\n",
    "(training,test) = movie_ratings.randomSplit([0.8,0.2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# tune model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Create ALS model\n",
    "model_als = ALS(userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", coldStartStrategy=\"drop\", seed=202112)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Define evaluator as RMSE\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Tune model using ParamGridBuilder\n",
    "params = ParamGridBuilder().addGrid(model_als.maxIter, [3, 5, 10]).addGrid(model_als.regParam, [0.1, 0.01, 0.001]).addGrid(model_als.rank, [5, 10, 15]).addGrid(model_als.alpha, [0.1, 0.01, 0.001]).build()\n",
    "\n",
    "# Build Cross validation\n",
    "cv_creator=CrossValidator(estimator=model_als,estimatorParamMaps=params,evaluator=evaluator,numFolds=4,seed=202112)\n",
    "\n",
    "#Fit ALS model to training data\n",
    "cv_model=cv_creator.fit(training)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ALS model parameters by CV:\n",
      "-> maxIter: 10\n",
      "-> regParam: 0.1\n",
      "-> rank: 5\n",
      "-> alpha: 0.1\n"
     ]
    }
   ],
   "source": [
    "best_model=cv_model.bestModel\n",
    "best_params=cv_model.getEstimatorParamMaps()[np.argmin(cv_model.avgMetrics)]\n",
    "print('Best ALS model parameters by CV:')\n",
    "for i,j in best_params.items():\n",
    "  print('-> '+i.name+': '+str(j))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# model tesing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error for testing data is 0.8861625353823233\n"
     ]
    }
   ],
   "source": [
    "#Generate predictions and evaluate using RMSE\n",
    "prediction_test = best_model.transform(test)\n",
    "rmse_test = evaluator.evaluate(prediction_test)\n",
    "print(\"Root-mean-square error for testing data is \" + str(rmse_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# qualitative check"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "# define a function to package the recommendation\n",
    "def top_K_recommend(k, uid, model, data_location=\"D:/py_movie_recommendation_system/data/\"):\n",
    "    \"\"\"\n",
    "    k: the number of movies to recommend\n",
    "    id: the id of the user to give recommendations\n",
    "    model: the trained model for recommendation\n",
    "    \"\"\"\n",
    "    # the table for all top10 recommendations\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"moive analysis\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "    movies_df = spark.read.load(data_location + \"movies.csv\", format='csv', header=True)\n",
    "\n",
    "    all_recommend = model.recommendForAllUsers(k)\n",
    "    user_recommend = all_recommend.where(all_recommend.userId == uid).toPandas()\n",
    "    if user_recommend.shape[0] == 0:\n",
    "        print('No user with id ' + str(uid) + ' is found in the data.')\n",
    "        print(\"Would you like to watch most frequently watched movie?\")\n",
    "        ratings_df = spark.read.load(data_location + \"ratings.csv\", format='csv', header=True)\n",
    "        movies_df.registerTempTable(\"movies\")\n",
    "        ratings_df.registerTempTable(\"ratings\")\n",
    "        out = spark.sql(\n",
    "            f\"\"\"\n",
    "            SELECT a.movieId, a.title, a.genres\n",
    "            FROM movies AS a\n",
    "            LEFT JOIN (\n",
    "            SELECT movieId, COUNT(rating) AS rating\n",
    "            FROM ratings\n",
    "            GROUP BY movieId\n",
    "            ) AS b\n",
    "            USING(movieId)\n",
    "            ORDER BY b.rating DESC\n",
    "            LIMIT {k}\n",
    "            \"\"\"\n",
    "        ).toPandas()\n",
    "        return out\n",
    "    user_recommend = user_recommend.iloc[0, 1]\n",
    "    user_recommend = pd.DataFrame(user_recommend, columns=['movieId', 'predicted_ratings'])\n",
    "    temp = None\n",
    "    for i in user_recommend['movieId']:\n",
    "        if not temp:\n",
    "            temp = movies_df.where(movies_df.movieId == str(i))\n",
    "        else:\n",
    "            temp = temp.union(movies_df.where(movies_df.movieId == str(i)))\n",
    "    out = pd.concat([temp.toPandas(), user_recommend['predicted_ratings']], axis=1)\n",
    "    out.index = range(1, k + 1)\n",
    "    return out\n",
    "\n",
    "\n",
    "# cosine similarity\n",
    "# the larger the cosine value, the smaller the two feature vectors' angle, the similar the movies\n",
    "# this similarity considers the direction only,\n",
    "# e.g. movie 1 with factor [1,2,3] and movie 2 with factor [2,4,6] are considered the same\n",
    "def cos_similar(k, mid, best_model, data_location=\"D:/py_movie_recommendation_system/data/\"):\n",
    "    \"\"\"\n",
    "    k: number of similar movies to find\n",
    "    mid: id of the movie to find similarities\n",
    "    \"\"\"\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"moive analysis\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "    movies_df = spark.read.load(data_location + \"movies.csv\", format='csv', header=True)\n",
    "    movies_df.registerTempTable(\"movies\")\n",
    "\n",
    "    movie_factors = best_model.itemFactors\n",
    "    movie_factors.printSchema()\n",
    "    comd = [\"movie_factors.selectExpr('id as movieId',\"]\n",
    "    for i in range(best_model.rank):\n",
    "        if i < best_model.rank - 1:\n",
    "            comd.append(\"'features[\" + str(i) + \"] as feature\" + str(i) + \"',\")\n",
    "        else:\n",
    "            comd.append(\"'features[\" + str(i) + \"] as feature\" + str(i) + \"'\")\n",
    "    comd.append(')')\n",
    "    movie_factors = eval(''.join(comd))\n",
    "    movie_factors.createOrReplaceTempView('movie_factors')\n",
    "\n",
    "    movie_info = spark.sql(\n",
    "        f\"\"\"\n",
    "        SELECT *\n",
    "        FROM movie_factors\n",
    "        WHERE movieId= {mid}\n",
    "        \"\"\"\n",
    "        ).toPandas()\n",
    "    if movie_info.shape[0] <= 0:\n",
    "        print('No movie with id ' + str(mid) + ' is found in the data.')\n",
    "        return None, None\n",
    "    norm_m = sum(movie_info.iloc[0, 1:].values ** 2) ** 0.5\n",
    "    temp = ['select movieId,']\n",
    "    norm_str = ['sqrt(']\n",
    "    for i in range(best_model.rank):\n",
    "        comd = 'feature' + str(i) + '*' + str(movie_info.iloc[0, i + 1])\n",
    "        temp.append(comd + ' as inner' + str(i) + ',')\n",
    "        if i < best_model.rank - 1:\n",
    "            norm_str.append('feature' + str(i) + '*feature' + str(i) + '+')\n",
    "        else:\n",
    "            norm_str.append('feature' + str(i) + '*feature' + str(i))\n",
    "    norm_str.append(') as norm')\n",
    "    temp.append(''.join(norm_str))\n",
    "    temp.append(' from movie_factors where movieId!=' + str(mid))\n",
    "    inner = spark.sql(' '.join(temp))\n",
    "    inner = inner.selectExpr('movieId',\n",
    "                             '(inner0+inner1+inner2+inner3+inner4)/norm/' + str(norm_m) + ' as innerP').orderBy(\n",
    "        'innerP', ascending=False).limit(k).toPandas()\n",
    "    out = None\n",
    "    for i in inner['movieId']:\n",
    "        if not out:\n",
    "            out = movies_df.where(movies_df.movieId == str(i))\n",
    "        else:\n",
    "            out = out.union(movies_df.where(movies_df.movieId == str(i)))\n",
    "    out = out.toPandas()\n",
    "    out.index = range(1, k + 1)\n",
    "    return out, inner\n",
    "\n",
    "\n",
    "\n",
    "# write a function to make prediction for movie id\n",
    "def similar_movie(k, mid, best_model, data_location=\"D:/py_movie_recommendation_system/data/\"):\n",
    "    out, inner = cos_similar(k, mid, best_model, data_location)\n",
    "    print(out)\n",
    "    if out is None:\n",
    "        print(\"This is a new movie, and we cannot find similar movie only based on it id!\")\n",
    "    return out\n",
    "\n",
    "# write a function to make recommendation\n",
    "def make_recommendation(k, uid, best_model, data_location=\"D:/py_movie_recommendation_system/data/\"):\n",
    "    out = top_K_recommend(k, uid, best_model, data_location)\n",
    "    print(out)\n",
    "    return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## for user"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programming\\python\\lib\\site-packages\\pyspark\\sql\\context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   movieId                                          title  \\\n",
      "1    84847                                    Emma (2009)   \n",
      "2     2693                                Trekkies (1997)   \n",
      "3    72171                          Black Dynamite (2009)   \n",
      "4     5485                                 Tadpole (2002)   \n",
      "5   158783                          The Handmaiden (2016)   \n",
      "6      446  Farewell My Concubine (Ba wang bie ji) (1993)   \n",
      "7    25771  Andalusian Dog, An (Chien andalou, Un) (1929)   \n",
      "8    26865     Fist of Legend (Jing wu ying xiong) (1994)   \n",
      "9     8477                               Jetée, La (1962)   \n",
      "10    7116            Diabolique (Les diaboliques) (1955)   \n",
      "\n",
      "                     genres  predicted_ratings  \n",
      "1      Comedy|Drama|Romance           5.485587  \n",
      "2               Documentary           5.298022  \n",
      "3             Action|Comedy           5.282207  \n",
      "4      Comedy|Drama|Romance           5.236969  \n",
      "5    Drama|Romance|Thriller           5.147257  \n",
      "6             Drama|Romance           5.136751  \n",
      "7                   Fantasy           5.136445  \n",
      "8              Action|Drama           5.128163  \n",
      "9            Romance|Sci-Fi           5.033184  \n",
      "10  Horror|Mystery|Thriller           5.024169  \n"
     ]
    }
   ],
   "source": [
    "# user in dataset\n",
    "out = make_recommendation(10, 500, best_model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "   movieId userId rating   timestamp  \\\n0      176    500    5.0  1005527755   \n1     1175    500    5.0  1005528017   \n2     1282    500    5.0  1005528236   \n3     1747    500    5.0  1005528065   \n4     1784    500    5.0  1005527784   \n..     ...    ...    ...         ...   \n81    1993    500    1.0  1005527364   \n82    2671    500    1.0  1005528078   \n83    3083    500    1.0  1005528017   \n84    3466    500    1.0  1005527926   \n85    4873    500    1.0  1005528205   \n\n                                               title  \\\n0                          Living in Oblivion (1995)   \n1                                Delicatessen (1991)   \n2                                    Fantasia (1940)   \n3                                 Wag the Dog (1997)   \n4                          As Good as It Gets (1997)   \n..                                               ...   \n81                             Child's Play 3 (1991)   \n82                               Notting Hill (1999)   \n83  All About My Mother (Todo sobre mi madre) (1999)   \n84                            Heart and Souls (1993)   \n85                                Waking Life (2001)   \n\n                                genres  \n0                               Comedy  \n1                 Comedy|Drama|Romance  \n2   Animation|Children|Fantasy|Musical  \n3                               Comedy  \n4                 Comedy|Drama|Romance  \n..                                 ...  \n81              Comedy|Horror|Thriller  \n82                      Comedy|Romance  \n83                               Drama  \n84                      Comedy|Fantasy  \n85             Animation|Drama|Fantasy  \n\n[86 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>movieId</th>\n      <th>userId</th>\n      <th>rating</th>\n      <th>timestamp</th>\n      <th>title</th>\n      <th>genres</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>176</td>\n      <td>500</td>\n      <td>5.0</td>\n      <td>1005527755</td>\n      <td>Living in Oblivion (1995)</td>\n      <td>Comedy</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1175</td>\n      <td>500</td>\n      <td>5.0</td>\n      <td>1005528017</td>\n      <td>Delicatessen (1991)</td>\n      <td>Comedy|Drama|Romance</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1282</td>\n      <td>500</td>\n      <td>5.0</td>\n      <td>1005528236</td>\n      <td>Fantasia (1940)</td>\n      <td>Animation|Children|Fantasy|Musical</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1747</td>\n      <td>500</td>\n      <td>5.0</td>\n      <td>1005528065</td>\n      <td>Wag the Dog (1997)</td>\n      <td>Comedy</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1784</td>\n      <td>500</td>\n      <td>5.0</td>\n      <td>1005527784</td>\n      <td>As Good as It Gets (1997)</td>\n      <td>Comedy|Drama|Romance</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>81</th>\n      <td>1993</td>\n      <td>500</td>\n      <td>1.0</td>\n      <td>1005527364</td>\n      <td>Child's Play 3 (1991)</td>\n      <td>Comedy|Horror|Thriller</td>\n    </tr>\n    <tr>\n      <th>82</th>\n      <td>2671</td>\n      <td>500</td>\n      <td>1.0</td>\n      <td>1005528078</td>\n      <td>Notting Hill (1999)</td>\n      <td>Comedy|Romance</td>\n    </tr>\n    <tr>\n      <th>83</th>\n      <td>3083</td>\n      <td>500</td>\n      <td>1.0</td>\n      <td>1005528017</td>\n      <td>All About My Mother (Todo sobre mi madre) (1999)</td>\n      <td>Drama</td>\n    </tr>\n    <tr>\n      <th>84</th>\n      <td>3466</td>\n      <td>500</td>\n      <td>1.0</td>\n      <td>1005527926</td>\n      <td>Heart and Souls (1993)</td>\n      <td>Comedy|Fantasy</td>\n    </tr>\n    <tr>\n      <th>85</th>\n      <td>4873</td>\n      <td>500</td>\n      <td>1.0</td>\n      <td>1005528205</td>\n      <td>Waking Life (2001)</td>\n      <td>Animation|Drama|Fantasy</td>\n    </tr>\n  </tbody>\n</table>\n<p>86 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT *\n",
    "    FROM ratings as r\n",
    "    LEFT JOIN movies as m\n",
    "    USING(movieId)\n",
    "    WHERE r.userId = 500\n",
    "    ORDER BY r.rating DESC\n",
    "    \"\"\"\n",
    ").toPandas()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programming\\python\\lib\\site-packages\\pyspark\\sql\\context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "c:\\programming\\python\\lib\\site-packages\\pyspark\\sql\\dataframe.py:138: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No user with id 1111111111111111 is found in the data.\n",
      "Would you like to watch most frequently watched movie?\n",
      "  movieId                                      title  \\\n",
      "0     356                        Forrest Gump (1994)   \n",
      "1     318           Shawshank Redemption, The (1994)   \n",
      "2     296                        Pulp Fiction (1994)   \n",
      "3     593           Silence of the Lambs, The (1991)   \n",
      "4    2571                         Matrix, The (1999)   \n",
      "5     260  Star Wars: Episode IV - A New Hope (1977)   \n",
      "6     480                       Jurassic Park (1993)   \n",
      "7     110                          Braveheart (1995)   \n",
      "8     589          Terminator 2: Judgment Day (1991)   \n",
      "9     527                    Schindler's List (1993)   \n",
      "\n",
      "                             genres  \n",
      "0          Comedy|Drama|Romance|War  \n",
      "1                       Crime|Drama  \n",
      "2       Comedy|Crime|Drama|Thriller  \n",
      "3             Crime|Horror|Thriller  \n",
      "4            Action|Sci-Fi|Thriller  \n",
      "5           Action|Adventure|Sci-Fi  \n",
      "6  Action|Adventure|Sci-Fi|Thriller  \n",
      "7                  Action|Drama|War  \n",
      "8                     Action|Sci-Fi  \n",
      "9                         Drama|War  \n"
     ]
    }
   ],
   "source": [
    "out = make_recommendation(10, 1111111111111111, best_model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## for movie"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = false)\n",
      " |-- features: array (nullable = true)\n",
      " |    |-- element: float (containsNull = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programming\\python\\lib\\site-packages\\pyspark\\sql\\dataframe.py:138: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\n",
      "c:\\programming\\python\\lib\\site-packages\\pyspark\\sql\\context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   movieId                                  title                 genres\n",
      "1     2311  2010: The Year We Make Contact (1984)                 Sci-Fi\n",
      "2     1221         Godfather: Part II, The (1974)            Crime|Drama\n",
      "3     3430                      Death Wish (1974)     Action|Crime|Drama\n",
      "4     1653                         Gattaca (1997)  Drama|Sci-Fi|Thriller\n",
      "5      357     Four Weddings and a Funeral (1994)         Comedy|Romance\n",
      "6     2333               Gods and Monsters (1998)                  Drama\n",
      "7    30848     Love Song for Bobby Long, A (2004)                  Drama\n",
      "8      936                       Ninotchka (1939)         Comedy|Romance\n",
      "9     3095            Grapes of Wrath, The (1940)                  Drama\n",
      "10    1945               On the Waterfront (1954)            Crime|Drama\n"
     ]
    }
   ],
   "source": [
    "# movie in the dataset\n",
    "out = similar_movie(10, 858, best_model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = false)\n",
      " |-- features: array (nullable = true)\n",
      " |    |-- element: float (containsNull = false)\n",
      "\n",
      "No movie with id 1111111 is found in the data.\n",
      "None\n",
      "This is a new movie, and we cannot find similar movie only based on it id!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programming\\python\\lib\\site-packages\\pyspark\\sql\\dataframe.py:138: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\n",
      "c:\\programming\\python\\lib\\site-packages\\pyspark\\sql\\context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# movie not in dataset\n",
    "out = similar_movie(10, 1111111, best_model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# save the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o71068.save.\n: java.lang.RuntimeException: java.io.FileNotFoundException: Could not locate Hadoop executable: C:\\Programming\\hadoop-3.3.1\\bin\\winutils.exe -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:736)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:271)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:287)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.hadoop.mapred.FileOutputCommitter.setupJob(FileOutputCommitter.java:131)\r\n\tat org.apache.hadoop.mapred.OutputCommitter.setupJob(OutputCommitter.java:265)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:178)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:79)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1578)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1578)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1564)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1564)\r\n\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:413)\r\n\tat org.apache.spark.ml.recommendation.ALSModel$ALSModelWriter.saveImpl(ALS.scala:539)\r\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:168)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.FileNotFoundException: Could not locate Hadoop executable: C:\\Programming\\hadoop-3.3.1\\bin\\winutils.exe -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBinInner(Shell.java:619)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1886)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1846)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1819)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:335)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:344)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:898)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_2488/2222829985.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mbest_model\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msave\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"D:/py_movie_recommendation_system/spark_warehouse\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mc:\\programming\\python\\lib\\site-packages\\pyspark\\ml\\util.py\u001B[0m in \u001B[0;36msave\u001B[1;34m(self, path)\u001B[0m\n\u001B[0;32m    224\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0msave\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpath\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    225\u001B[0m         \u001B[1;34m\"\"\"Save this ML instance to the given path, a shortcut of 'write().save(path)'.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 226\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msave\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    227\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    228\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\programming\\python\\lib\\site-packages\\pyspark\\ml\\util.py\u001B[0m in \u001B[0;36msave\u001B[1;34m(self, path)\u001B[0m\n\u001B[0;32m    175\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    176\u001B[0m             \u001B[1;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"path should be a string, got type %s\"\u001B[0m \u001B[1;33m%\u001B[0m \u001B[0mtype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 177\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_jwrite\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msave\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    178\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    179\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0moverwrite\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\programming\\python\\lib\\site-packages\\py4j\\java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1307\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1308\u001B[0m         \u001B[0manswer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1309\u001B[1;33m         return_value = get_return_value(\n\u001B[0m\u001B[0;32m   1310\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[0;32m   1311\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\programming\\python\\lib\\site-packages\\pyspark\\sql\\utils.py\u001B[0m in \u001B[0;36mdeco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    109\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0ma\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    110\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 111\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0ma\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    112\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    113\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\programming\\python\\lib\\site-packages\\py4j\\protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[1;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[0;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    325\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 326\u001B[1;33m                 raise Py4JJavaError(\n\u001B[0m\u001B[0;32m    327\u001B[0m                     \u001B[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n",
      "\u001B[1;31mPy4JJavaError\u001B[0m: An error occurred while calling o71068.save.\n: java.lang.RuntimeException: java.io.FileNotFoundException: Could not locate Hadoop executable: C:\\Programming\\hadoop-3.3.1\\bin\\winutils.exe -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:736)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:271)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:287)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.hadoop.mapred.FileOutputCommitter.setupJob(FileOutputCommitter.java:131)\r\n\tat org.apache.hadoop.mapred.OutputCommitter.setupJob(OutputCommitter.java:265)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:178)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:79)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1578)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1578)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1564)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1564)\r\n\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:413)\r\n\tat org.apache.spark.ml.recommendation.ALSModel$ALSModelWriter.saveImpl(ALS.scala:539)\r\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:168)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.FileNotFoundException: Could not locate Hadoop executable: C:\\Programming\\hadoop-3.3.1\\bin\\winutils.exe -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBinInner(Shell.java:619)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1886)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1846)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1819)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:335)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:344)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:898)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\n"
     ]
    }
   ],
   "source": [
    "best_model.save(\"D:/py_movie_recommendation_system/spark_warehouse\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}